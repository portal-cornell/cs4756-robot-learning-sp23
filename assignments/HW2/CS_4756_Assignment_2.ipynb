{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "Welcome to your second coding Assignment of CS4756/5756. In this short notebook, you will implement Model Predictive Control for the Pendulum environment using the Cross-Entropy Method for Trajectory Optimization.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness, and for certain assignments, speed. Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [#resources](https://www.cs.cornell.edu/courses/cs4756/2023sp/#resources) section on the course website is your friend (especially for this first assignment)! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. Since this is the first iteration of this course, please do not hesitate to reach out to TAs if you find any errors in the assignments. "
      ],
      "metadata": {
        "id": "HNeuKk3CA84D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "rG5ASo7-AEU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym[classic_control] > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "gWwSVqhjA84O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "import random"
      ],
      "metadata": {
        "id": "nAXoH_GXyoTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the environment\n"
      ],
      "metadata": {
        "id": "AHttcjbb1VxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Pendulum-v1', g=9.81)\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    screen = env.render(mode='rgb_array')\n",
        "\n",
        "    plt.imshow(screen)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "-LO_J5dtwQYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CEM_Agent():\n",
        "    def __init__(self, \n",
        "                 action_dim = None, \n",
        "                 planning_horizon = None, \n",
        "                 num_rollouts=None, \n",
        "                 proportion_elite = None, \n",
        "                 sigma_init = None, \n",
        "                 noise = None, \n",
        "                 num_iter = None,\n",
        "                 learned_transition_model = None,\n",
        "                 action_high = None,\n",
        "                 action_low = None):\n",
        "        self.action_dim = action_dim\n",
        "        self.planning_horizon = planning_horizon\n",
        "        self.num_rollouts = num_rollouts\n",
        "        self.proportion_elite = proportion_elite\n",
        "        self.sigma_init = sigma_init\n",
        "        self.noise = noise\n",
        "        self.num_iter = num_iter\n",
        "        self.learned_transition_model = learned_transition_model\n",
        "        self.rollout_env = gym.make('Pendulum-v1', g=9.81)\n",
        "        self.action_high = action_high\n",
        "        self.action_low = action_low\n",
        "\n",
        "        self.action_means = None\n",
        "        self.action_sigmas = None\n",
        "    \n",
        "    def get_full_state(self, obs):\n",
        "        x, y, thetadot = obs\n",
        "        theta = np.arctan2(y, x)\n",
        "        return np.array([theta, thetadot])\n",
        "\n",
        "    def custom_step(self, action, obs):\n",
        "        # Function returns next state, reward for transition and whether or not the episode has terminated\n",
        "        if self.learned_transition_model is not None:\n",
        "            \"\"\"TODO for Extra Credit: Use a learned transition model to step the environment during rollouts\n",
        "            Outputs:\n",
        "            ----------\n",
        "            obs: next state \n",
        "            reward: reward for the next state\n",
        "            done: whether the episode has terminated\n",
        "            \"\"\"\n",
        "            pass\n",
        "        else:\n",
        "            self.rollout_env.reset()\n",
        "            self.rollout_env.state = self.rollout_env.unwrapped.state = self.get_full_state(obs)\n",
        "            obs, reward, done, info = self.rollout_env.step(action)\n",
        "            return obs, reward, done \n",
        "\n",
        "    def reset_distribution(self):\n",
        "        \"\"\"TODO: Resets the action distribution (sets self.action_means and self.action_sigmas)\"\"\"\n",
        "        pass\n",
        "\n",
        "    def update_distribution(self, current_state, iter_num=None):\n",
        "        \"\"\"TODO: Update the action distribution by sampling rollouts from action distribution and selecting best rollouts\n",
        "\n",
        "        Inputs\n",
        "        ----------\n",
        "        current_state : The current state of the environment\n",
        "        iter_num (optional): The number of times the distrubution has been updated (can be useful for adding noise to variance updates)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def rollout_plans(self, current_state):\n",
        "        \"\"\"TODO: Samples and rolls out on Pedulum environment\n",
        "\n",
        "        Inputs\n",
        "        ----------\n",
        "        current_state : The current state of the environment\n",
        "\n",
        "        Outputs\n",
        "        ----------\n",
        "        rollouts: Dimension (num_rollouts, planning_horizon, action_dim) - plans sampled from action distributions\n",
        "        rollout_costs: A list of costs for each rollout\n",
        "        \"\"\"\n",
        "        pass     \n",
        "\n",
        "    def rollout_cost(self, rollout_plan, current_state):\n",
        "        \"\"\"TODO: Rolls out an action plan on the Pendulum environment from the current state\n",
        "\n",
        "        Inputs\n",
        "        ----------\n",
        "        rollout_plan: Dimension (planning_horizon, action_dim) - a single plan sampled from the action distribution\n",
        "        current_state : The current state of the environment\n",
        "\n",
        "        Outputs\n",
        "        ----------\n",
        "        rollout_cost: Cost of the rollout\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def mpc_policy(self, current_state):\n",
        "        \"\"\"TODO: Executes the CEM updates and returns the action to be executed on the current timestep\n",
        "\n",
        "        Inputs\n",
        "        ----------\n",
        "        current_state: The current state of the environment\n",
        "\n",
        "        Outputs\n",
        "        ----------\n",
        "        action: The first sampled action in the planning horizon of the action distribution\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "s-7oAemdiagD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Initialize the CEM agent with appropriate hyperparameters\n",
        "cem_agent = CEM_Agent(action_dim = None, \n",
        "                 planning_horizon = None, \n",
        "                 num_rollouts=None, \n",
        "                 proportion_elite = None, \n",
        "                 sigma_init = None, \n",
        "                 noise = None, \n",
        "                 num_iter = None,\n",
        "                 learned_transition_model = None,\n",
        "                 action_high = None,\n",
        "                 action_low = None)"
      ],
      "metadata": {
        "id": "nXDHjMpCBlav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_policy(cem_agent, render = False):\n",
        "    env = gym.make('Pendulum-v1', g=9.81)\n",
        "    current_state = env.reset()\n",
        "    total_reward = 0\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    if render:\n",
        "        prev_screen = env.render(mode='rgb_array')\n",
        "        plt.imshow(prev_screen)\n",
        "\n",
        "    for i in range(50):\n",
        "        action = cem_agent.mpc_policy(current_state)\n",
        "        current_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if render:\n",
        "            screen = env.render(mode='rgb_array')\n",
        "            plt.imshow(screen)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward, time.time()-start_time"
      ],
      "metadata": {
        "id": "7asO-jOT25EF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}